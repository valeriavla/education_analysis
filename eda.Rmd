
```{r}

library(tidyverse)
library(ggplot2)
library(plyr)


df <- read.csv("../enrollement_completer.csv")
df1 <- read.csv("../grad_completion_ratesData_11-16-2022---690.csv")

# clean up column names
colnames(df1) <- gsub("\\.\\.", "_", tolower(gsub("Graduation.rate...Bachelor.degree.within.6.years..", "gradrate_ba_6yrs", gsub("..DRVGR2020_RV.", "", colnames(df1)))))
colnames(df) <- gsub("..EF2013A..All.students..Undergraduate.total.", "undegrad_enrollment_count_2013", tolower(colnames(df)))

# combine graduation rates with the base dataframe
df <- merge(df, df1, by = 'unitid', all.x = TRUE)
colnames(df) <- gsub("\\.$", "", colnames(df))
print(nrow(df))

# convert the sector into the actual names
df$sector <- mapvalues(df$sector, from = c(1, 2, 3), to = c("Public", "Private not-for-profit", "Private for-profit"))

write.csv(df, "all_data_merged_df.csv", row.names = FALSE)

```



```{r}
# graduation rates histogram
hist(df[['gradrate_ba_6yrstotal']],
     main="Histogram of Graduation Rates for\n4-Year Colleges in the United States",
     xlab="Graduation Rate")


boxplot(df[['gradrate_ba_6yrstotal']],
        main="Boxplot of All 4-Year Colleges in the U.S.",
        ylab="Graduation Rate")
```

```{r}

boxplot(df[['gradrate_ba_6yrsmen']],
        df[['gradrate_ba_6yrswomen']],
        names=c("Men Graduation Rate", "Women Graduation Rate"),
        main="Boxplot of All 4-Year Colleges in the U.S. by Gender",
        xlab='Gender',
        ylab="Graduation Rate")
abline(h=df[['gradrate_ba_6yrstotal']])


# make data in long format so that we can see the grad rate for men / women separately
# long_df <- df %>% dplyr::select(unitid, gradrate_ba_6yrsmen, gradrate_ba_6yrswomen) %>% pivot_longer(!unitid, names_to="gender", values_to="rate")

# long_df %>% ggplot(aes(x = gender, y=rate, fill=gender)) + geom_histogram(position="dodge", stat="identity") + ggtitle("Histogram of Different Graduation Rates for 4-Year Colleges by Gender") + ylab("4-Year College Gradution Rate") + xlab("Gender") 
```

```{r}
# scatterplot to see graduation rate of all students vs. pell recipients
plot(df[['gradrate_ba_6yrstotal']], df[['pell.grant.recipients_.overall.graduation.rate.within.150.percent.of.normal.time']], main="Scatterplot of Overall Graduation Rate vs.\nGraduation Rate for Pell Recipients",
   xlab="6-Year Graduation Rate for All Students 4-Year Colleges", ylab="6-Year Graduation Rate for Pell Grant Recipients", pch=19)
abline(b = 1, a = 0, col='red')
```

```{r}
# any points above the red dotted lines are schools where Pell-Grant recipients are graduating at *higher* rates than the general student body
# we added the dimension of school size to the mix and as you can see, majority of the schools where Pell-Grant recipients are graduating at much higher rates than the general student body are smaller sized schools
ggplot(df, aes(x=gradrate_ba_6yrstotal, y=pell.grant.recipients_.overall.graduation.rate.within.150.percent.of.normal.time, color = sector)) + 
  geom_point(aes(size=tot_enroll_2013)) + geom_abline(intercept=0, slope=1, linetype='dashed', color='red') + labs(y='Pell Recipients 6-Year Graduation Rate', x='6-Year Graduation Rate All Students') + ggtitle("Comparing Overall Graduation Rates to\nPell Recipients Graduation Rates")

```

```{r}
# any points above the red dotted lines are schools where women graduate at higher rates than men
ggplot(df, aes(y=gradrate_ba_6yrswomen, x=gradrate_ba_6yrsmen, color = sector)) + 
  geom_point(aes(size=tot_enroll_2013)) + geom_abline(intercept=0, slope=1, linetype='dashed', color='red') + labs(x='6-Year Graduation Rate for Men', y='6-Year Graduation Rate for Women') + ggtitle("Comparing Graduation Rates of Men vs. Women")

```

```{r}
# any points above the red dotted lines are schools where black students graduate at higher rates than white students
ggplot(df, aes(y=gradrate_ba_6yrsblack_non.hispanic, x=gradrate_ba_6yrswhite_non.hispanic, color = sector)) + 
  geom_point(aes(size=tot_enroll_2013)) + geom_abline(intercept=0, slope=1, linetype='dashed', color='red') + labs(x='6-Year Graduation Rate for White Students', y='6-Year Graduation Rate for Black Non-Hispanic Students') + ggtitle("Comparing Graduation Rates of White and Black Students")

```

```{r}
# any points above the red dotted lines are schools where hispanic students graduate at higher rates than white students
ggplot(df, aes(y=gradrate_ba_6yrshispanic, x=gradrate_ba_6yrswhite_non.hispanic, color = sector)) + 
  geom_point(aes(size=tot_enroll_2013)) + geom_abline(intercept=0, slope=1, linetype='dashed', color='red') + labs(x='6-Year Graduation Rate for White Non-Hispanic Students', y='6-Year Graduation Rate for Hispanic Students') + ggtitle("Comparing Graduation Rates of White and Hispanic Students")

```


```{r}
id_cols <- c("unitid", "institution.name.x", "institution_entity_name_hd2020", "state.abbreviation_hd2020", "city.location.of.institution_hd2020",
             "zip.code_hd2020", "institutional.category_hd2020")
full_grad_rate <- df %>% subset(gradrate_ba_6yrstotal == 100) %>% dplyr::select(append(id_cols, colnames(df)[grep("grad(uation.)?rate", colnames(df))]))
```


`Perform a Hypothesis test to answer the above data science question.`

c. Write your Null and Alternative Hypothesis. 

> $H_0:$ The average danceability of all of Ariana Grande's songs is *less than or equal to* the average danceability of all of Kelly Clarkson's songs.

> $H_a:$ The average danceability of all of Ariana Grande's songs is *greater than* the average danceability of all of Kelly Clarkson's songs.

d. Do a t-test (if your Data science question is about population _averages_. If your question is about comparing _proportions_ then use a Z-test), and write your conclusion at 5\% significance level.

```{r}
kelly_dance <- kelly[['kc.danceability']]
ari_dance <- ari[['ag.danceability']]
t.test(ari_dance,
       kelly_dance,
       alternative="greater")

```

> The p-value is less than 0.05 which means that at a 5% significance level, we have enough evidence to reject the null hypothesis. Therefore there is strong enough evidence to show that on average, the danceability of Ariana Grande's songs is greater than the average danceability of Kelly Clarkson's songs. We also see that the confidence interval is between 0.015 to infinity, showing that we are 95% confident that Ariana's songs are on average at least 0.015 more danceable than Kelly's.


f. Compare your results from part d and e.

> Both my results from part d and e agree that on average, Ariana Grande's songs are more danceable than Kelly Clarkson. We see that boostrapping the data resulted in a more precise measurement of how much more danceable Ariana's songs are than Kelly Clarksons. From just the t-test alone without bootstrapping, we could be 95% confident that Ariana's songs were on average at least 0.062 more danceable than Kelly's, but after bootstrapping, we were able to be 95% confidence that Ariana's songs were on average 0.059 to 0.093 more danceable 
  
g. Write two broader topics that could be analysed using these millions of data provided by Spotify. (Can get some idea from here : <https://research.atspotify.com/datasets/>)

> Topic 1: I would want to dig into whether seasonal changes resulted in changes in listening patterns. For example, I'd want to see whether there were significant differences in average tempo, energy, acousticness, etc. of songs listened to during the Summer vs. the Winter vs. the Fall and Spring. I could also dig into whether this differs depending how "temperate" the seasons are in that part of the globe. If we were able to pair this up with climate data over time for a specific city, we could even get more granular with our analysis and see whether changes in weather within a specific day (rather than a whole season) triggers changes in listening patterns. 

> Topic 2: I would want to see whether I could find a correlation between the Podcasts topics and music genre someone listens to. I often meet other people who listen to the same kinds of podcasts that I'm interested in (daily news, pop culture, and knowledge), who also listen to the same kind of music I listen to. It would be interesting to see if there was a certain pattern of podcasts that correlates often with R&B, or Country, or Pop. Some of this might be due to the backgrounds of podcasts hosts outside of their life as a podcaster (e.g. many people probably listen to Meghan Markle's podcast because of who she is rather than the content of the episodes), or it might even have to do with the surrounding culture around a genre that might point to certain political views and podcasts, and even this would be interesting to see if it could be captured through the data.



> In order to see whether Offense Type is independent of Offender's race, I will use a Chi square test of indepdence. 

b. Do EDA relevant to this question. What can you observe from your Data Vises?

```{r}
library(readxl)
library(dplyr)
crime <- read_excel(path = "data/Table_3_Offenses_Known_Offenders_Race_and_Ethnicity_by_Offense_Type_2020.xlsx",
                    skip = 5) %>% select(c("Offense type",
                                           "White",
                                           "Black or African American",
                                           "American Indian or Alaska Native",
                                           "Asian",
                                           "Native Hawaiian or Other Pacific Islander",
                                           "Multiple",
                                           "Unknown Race")) %>% dplyr::rename(offense_type=`Offense type`) %>% filter(offense_type %in% c("Crimes against persons:", "Crimes against property:", "Crimes against society3"))

crime <- data.frame(crime)

```


```{r fig.align="center", fig.width = 8}
crimes_df <- crime %>% pivot_longer(!offense_type,
                                     names_to="race",
                                     values_to="count")

crimes_df %>% ggplot(aes(x = offense_type, y=count, fill=race)) + geom_bar(position="fill", stat="identity") + ggtitle("Different Offense Types and their\nRacial Breakdown by Known Offender's Race") + ylab("Percentage of Crimes Committed by Each Race") + xlab("Crime Type") 

crimes_df %>% ggplot(aes(x = offense_type, y=count, fill=race)) + geom_histogram(position="dodge", stat="identity") + ggtitle("Histogram of Different Offense Types and Race of Known Offender") + ylab("Number of Crimes Committed by Each Race") + xlab("Crime Type") 
```

> From these data visualizations, a few things pop out to me:

* For all 3 types of crime, the majority (> 50%) of the crimes were committed by a White offender. 
* The most frequenty type of crime is crimes against persons by **a lot**. Even just the crimes committed by White offenders against other persons already has more cases than the other two crime types combined. 
* For crimes against property, about 25% of the cases do not have a known offender's race.
* The distribution of races is pretty consistent for all 3 crime types, except for: For crimes against property, about 25% of cases do not have a known offenders race, in comparison to much smaller percentages (~5-10%) for the other 2 crime categories. For crimes against society, about 25% of cases were committed by a Black offender, in comparison to smaller percentages (~10-15%) for the other 2 crime categories. 

c. What is the Null and Alternative Hypothesis?

> $H_0$: There is no relation between offense type and known offender's race.

> $H_a$: There is a relationship between offense type and known offender's race.

d. From your test, what is the conclusion at 5\% significance level?
```{r}
# set up the crime dataframe so that the offense types are the index and the columns are the different races
crime_table <- crime
rownames(crime_table) <- crime_table$offense_type
crime_table <- crime_table %>% select(-offense_type)

(chisq.test(crime_table))
(actual_chi <- chisq.test(crime_table)$statistic)
```


> Our p-value is significantly less than 0.05 which means that at the 5% significance level, we have enough evidence to reject the null hypothesis and thus enough evidence to show that there is a relationship between offense type and known offender's race. 

e. As an impartial data scientist, what can you say about what you have observed above from the statistical test?

> Although my p-value was less than 0.05, as a data scientist I would not be confident in actually declaring a relationship between offense type and the race of the offender. I see a few things wrong with the way the data was given: first off - we always have to consider the domain of the data science question being posed. In this case, any data dealing with crime is going to have pre-existing embedded biases due to the unjust nature of the criminal justice system which convicts people of color (specifically people belonging to the races Black or African American and American Indian people) at higher rates. Secondly - we were given raw counts of crimes committed without data indicating the population proportion for each race. From our EDA portion, we see that White people make up the largest proportion of each offense type, but that is because majority of the American population is white (75.8% according to 2021 population estimates (source: https://www.census.gov/quickfacts/fact/table/US/PST045221)). Because this population proportion is so heavily imbalanced, these raw counts might impact the Chi-Squared Test of Independence. So even though the Chi-Squared test that I ran resulted in a p-value of 0.05, I still would not make the claim that race and offense type have a relation - without a statistical test that factors in population distribution and without background context on the nature of the issue.

## Problem 3

a. Explain 1 example where Maximum Likelihood Estimation is used for estimating the parameters of statistical models; _models used in ML_. Please use your own words for explanation. 

You can refer to any articles/papers/online examples. If you are using online examples, don't forget to refer the link here and explain how the MLE procedure is being used and explain the steps of MLE calculation. 

(You don't have to go into great details, but explain these examples assuming that you are a TA for 511 in the Fall 2023  and you are explaining this to a new student)

![](MLE_example1.png)

> My first example of using Maximum Likelihood Estimation to estimate the parameters of a statistical model comes from an example that we did in our 501 class for our Lab 3.1. We were given a dataset that listings of houses that provided their price and their distance in miles from the city center (as shown in the plot above). We wanted to create an ML model (in this case a Linear Regression) which explained the relationship between these two variables and thus would be able to predict the price of a house given the distance from the city center. We wanted to use Maximum Likelihood Estimation to estimate the parameters of the distribution of the data observations. In this process, first you have to make an estimated guess about what kind of distribution this data came from. From looking at the data, a good guess was that this came from a double gaussian distribution. After we established that, we then wanted to begin estimating the parameters that went into generating the data distribution. To do this, we want to maximize the conditional probability of observing these data points, given a specific probability distribution that we define by the parameters we're guessing. Since we know that the probability of many independent things happening (and we're assuming all of the house listings are independent) is the product of each probability, we can transform this product by taking the log of it, and thus the product becomes a sum of log-likelihoods. From there we can use embedded Python packages such as scikitlearn or numpy to iterate through all parameters until we eventually find the parameters that maximize the probability function. Another mathematical way that we could find the maximum could be by getting the derivative of the function and find where the derivative equals 0 to see if it's a local or global maximum.

b. Write another model/place where Maximum Likelihood Estimation is used for estimating the parameters (again a model used in ML) but you don't have to explain the steps.

> Another ML model which uses Maximum Likelihood Estimation is in Time Series. 

> Source: https://www.webpages.uidaho.edu/~chrisw/stat550/Maximum%20Likelihood%20Estimation%20in%20Time%20Series.pdf


## Problem 4 (Bonus-10 points)

(Self-learn Question: MANOVA)

(It is okay to use the internet to find answers but please put links to all your references and try to write the answers on your words as much as possible)

a. In a few sentences, in your words, explain where/when/in what situations MANOVA is being used. What assumptions are used in this test?

> You would want to use a MANOVA test in situations where you want statistical evidence on whether there are differences between one or more indepdendent groups and on more than one continuous dependent variables. If you had only *one* dependent variables, you would want to use an ANOVA test instead. Some of these situations could involve surveying different groups of people to see if some characteristics about them (be it demographic, or background) has an affect on their perception, viewpoint, or a life outcome. One example might be using a MANOVA to understand whether there were differences in a student's ability to remember things from lecture in the short-term (after 1 day) and in the long-term (at the end of the semester), based off of whether they attended a short lecture (30 min), medium lecture (60 min), long lecture (90 min), or lecture+lab (120 min). In this case our dependent variables are the short-term memory recall and long-term memory recall, and our independent variables are our lecture length. 

> Source: https://statistics.laerd.com/spss-tutorials/one-way-manova-using-spss-statistics.php#:~:text=For%20example%2C%20you%20could%20use,independent%20variable%20is%20%22drug%20users

> There are a few assumptions that need to be true in order to correctly use a MANOVA test:

* Your dependent variables need to be continuous - that is, each of your dependent variable categories are just different levels of each other. In the example I used above, they were the # of facts recalled after different time intervals.
* Your observations should all be independent of each other, thus there should not be more than one row that represents the same sample. 
* Your independent variables need to be categorical, and independent of each other. 
* Your dependent variables need to be multivariate normally distributed - meaning that for each of the independent groups, the distribution of the dependent variable needs to be approximately normal.
* The population covariance matrices of each group need to be the same.
* For each indepdent group, the dependent variables need to have a linear relationship with the independent variables
* There needs to be no outliers in the data

> Source: https://www.real-statistics.com/multivariate-statistics/multivariate-analysis-of-variance-manova/manova-assumptions/#:~:text=In%20order%20to%20use%20MANOVA,independent%20variables%20(which%20are%20categorical)


b. Demonstrate one example where you would use this test in R. Write your conclusions.

> An example where we would use this test in R is analyzing different types of variations of plants (such as the pink princess being a variety of the Philodendron), and looking at their associated plant heights and canopy volume. We can use the MANOVA test to see if plant heights and canopy volume are associated with different plant varieties. 
> In this example (source below), the MANOVA test resulted in showing that there is a large effect of plant varieties on both plant height and canopy volume.

> Source: https://www.reneshbedre.com/blog/manova.html
